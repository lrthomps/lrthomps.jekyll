<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    
      <!--<script src="https://tex.s2cms.ru/latex.js"></script>-->
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    
<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Learning to learn without forgetting: a summary | Lara Thompson</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="Learning to learn without forgetting: a summary" />
<meta name="author" content="Lara Thompson" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Matthew Riemer et. al. Learning to Learn Without Forgetting by Maximizing Transfer and Minimizing Interference. ICLR, 2019. Recall catastrophic forgetting, a neural network sequentially trained on multiple tasks forgets earlier tasks with each new task, apparently not a problem in Bayesian networks Why? overwriting weights with updates, … How to avoid? limit weight sharing, balance network stability vs plasticity (&quot;recall of old tasks&quot; versus &quot;rapid learning of new ones&quot;), … The loss function: $$\sum_{i, j} L(x_i , y_i ) + L(x_j , y_j ) − \alpha {\partial L(x_i , y_i ) \over \partial \theta} \cdot {\partial L(x_j , y_j ) \over \partial \theta}$$ The regularizing term is a measure of transfer or interference between updates. The gradient wrt to learning parameters guides the backprop update to those parameters: alignment of gradients means the updates agree and will guide learning for both examples; anti-alignment means updates cancel and neither example will learn; any intervening overlap is deemed transfer (interference) for positive (negative) values. Maximizing weight sharing maximizes transfer; minimizing weight sharing minimizes the change for interference. Work leading up to this paper, both offline algorithms over dataset D: MAML -&gt; FOMAML (Finn &amp; Levine, 2017) Reptile (Nichol &amp; Schulman, 2018) Contributions: new algorithm MER, meta experience replay, an online algorithm (algorithms 1 with variants 6 &amp; 7): added an inner loop within Reptile batches for an inner meta-learning update keeps a memory/reservoir of examples M to approximate the full dataset D with new examples added probabilistically to replace old ones (see algorithm 3 in the paper) prioritizes learning of the current examples, esp. because it may not be saved First, the reptile algorithm: for each epoch of training, \(t\), record the current params, \(\theta^A_0 = \theta_{t-1}\) and sample \(s\) batches of size \(k\) perform a normal epoch of training over the \(s\) batches with learning rate \(\alpha\) toward final params \(\theta^A_s\) update the network weights for this epoch only a fraction of the learned param changes: $$\theta_t = \theta^A_0 + \gamma (\theta^A_s - \theta^A_0)$$ this meta-learning update enacts the effective loss $$2\sum_{i=1}^s L(B_i) - \sum_{j=1}^{i-1} {\partial L(B_i) \over \partial \theta} \cdot {\partial L(B_j) \over \partial \theta}$$ MER adds a second meta-learning update within each of the \(s\) batches, now sampled from reservoir M, each of which will have the current example in it; finally, the reservoir is updated (maybe) for each epoch of training, \(t\), record the current params, \(\theta^A_0 = \theta_{t-1}\) and sample \(s\) batches of size \(k\), include example \(x_t, y_t\) in each for each batch \(i\), record the current params, \(\theta^A_{i, 0} = \theta^A_{i-1} \) for each example \(j\) in the batch, perform a backprop update with learning rate \(\alpha\) to params \(\theta^A_{i, j}\) after the entire batch has been singly learned, meta-learn the parameter update $$\theta^A_i = \theta^A_{i, 0} + \beta (\theta^A_{i, k} - \theta^A_{i, 0})$$ the effective loss is $$2\sum_{i=1}^s \sum_{j=1}^k L(x_{ij}, y_{ij}) - \sum_{q=1}^{i-1}\sum_{r=1}^{j-1} {\partial L(x_{ij}, y_{ij}) \over \partial \theta} \cdot {\partial L(x_{qr}, y_{qr}) \over \partial \theta}$$ note that they update the batch examples singly to maximize the regularizing effect algorithms 6 &amp; 7 are alternate ways of prioritizing the current example Evaluation metrics: learning accuracy (LA): average accuracy for each task immediately after it has been learned retained accuracy (RA): final retained accuracy across all tasks learned sequentially backward transfer and interference (BTI): the average change in accuracy from when a task is learned to the end of training (positive good; large and negative is catastrophic forgetting) Problems: in supervised learning: MNIST permutations, each task is transformed by a fixed permutation of the MNIST pixels; MNIST rotations, each task contains digits rotated by a fixed angle between 0 and 180 degrees; Omniglot, each task is one of 50 alphabets with overall 1623 characters in reinforcement learning: Catcher, a board moved left/right to catch a more and more rapidly falling object; Flappy Bird must fly between ever tightening pipes Compared against: online, same network trained straightforwardly one example at a time on the incoming non-stationary training data by simply applying SGD independent, one model per task with size of network reduced proportionally to keep total number of parameters fixed task input, trained as in online with a dedicated input layer per task EWC, Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017), ~online regularized to avoid catastrophic forgetting GEM: Gradient Episodic Memory (GEM) (Lopez-Paz &amp; Ranzato, 2017) uses episodic storage to modify gradients of latest example to not interfere with past ones; stored examples are not used in ongoing training Findings: MER seems to do learn and retain the most over all tasks, faster, and with less memory my reservations: mnist again? omniglot is not usually studied with any of the algorithms compared against: in Lake (2015) they achieve &lt;5% error rate, still &lt;15% in a stripped down version of their model and 2 out of 3 of their baselines how much slower will the training be with single example batches and two meta-learning updates?" />
<meta property="og:description" content="Matthew Riemer et. al. Learning to Learn Without Forgetting by Maximizing Transfer and Minimizing Interference. ICLR, 2019. Recall catastrophic forgetting, a neural network sequentially trained on multiple tasks forgets earlier tasks with each new task, apparently not a problem in Bayesian networks Why? overwriting weights with updates, … How to avoid? limit weight sharing, balance network stability vs plasticity (&quot;recall of old tasks&quot; versus &quot;rapid learning of new ones&quot;), … The loss function: $$\sum_{i, j} L(x_i , y_i ) + L(x_j , y_j ) − \alpha {\partial L(x_i , y_i ) \over \partial \theta} \cdot {\partial L(x_j , y_j ) \over \partial \theta}$$ The regularizing term is a measure of transfer or interference between updates. The gradient wrt to learning parameters guides the backprop update to those parameters: alignment of gradients means the updates agree and will guide learning for both examples; anti-alignment means updates cancel and neither example will learn; any intervening overlap is deemed transfer (interference) for positive (negative) values. Maximizing weight sharing maximizes transfer; minimizing weight sharing minimizes the change for interference. Work leading up to this paper, both offline algorithms over dataset D: MAML -&gt; FOMAML (Finn &amp; Levine, 2017) Reptile (Nichol &amp; Schulman, 2018) Contributions: new algorithm MER, meta experience replay, an online algorithm (algorithms 1 with variants 6 &amp; 7): added an inner loop within Reptile batches for an inner meta-learning update keeps a memory/reservoir of examples M to approximate the full dataset D with new examples added probabilistically to replace old ones (see algorithm 3 in the paper) prioritizes learning of the current examples, esp. because it may not be saved First, the reptile algorithm: for each epoch of training, \(t\), record the current params, \(\theta^A_0 = \theta_{t-1}\) and sample \(s\) batches of size \(k\) perform a normal epoch of training over the \(s\) batches with learning rate \(\alpha\) toward final params \(\theta^A_s\) update the network weights for this epoch only a fraction of the learned param changes: $$\theta_t = \theta^A_0 + \gamma (\theta^A_s - \theta^A_0)$$ this meta-learning update enacts the effective loss $$2\sum_{i=1}^s L(B_i) - \sum_{j=1}^{i-1} {\partial L(B_i) \over \partial \theta} \cdot {\partial L(B_j) \over \partial \theta}$$ MER adds a second meta-learning update within each of the \(s\) batches, now sampled from reservoir M, each of which will have the current example in it; finally, the reservoir is updated (maybe) for each epoch of training, \(t\), record the current params, \(\theta^A_0 = \theta_{t-1}\) and sample \(s\) batches of size \(k\), include example \(x_t, y_t\) in each for each batch \(i\), record the current params, \(\theta^A_{i, 0} = \theta^A_{i-1} \) for each example \(j\) in the batch, perform a backprop update with learning rate \(\alpha\) to params \(\theta^A_{i, j}\) after the entire batch has been singly learned, meta-learn the parameter update $$\theta^A_i = \theta^A_{i, 0} + \beta (\theta^A_{i, k} - \theta^A_{i, 0})$$ the effective loss is $$2\sum_{i=1}^s \sum_{j=1}^k L(x_{ij}, y_{ij}) - \sum_{q=1}^{i-1}\sum_{r=1}^{j-1} {\partial L(x_{ij}, y_{ij}) \over \partial \theta} \cdot {\partial L(x_{qr}, y_{qr}) \over \partial \theta}$$ note that they update the batch examples singly to maximize the regularizing effect algorithms 6 &amp; 7 are alternate ways of prioritizing the current example Evaluation metrics: learning accuracy (LA): average accuracy for each task immediately after it has been learned retained accuracy (RA): final retained accuracy across all tasks learned sequentially backward transfer and interference (BTI): the average change in accuracy from when a task is learned to the end of training (positive good; large and negative is catastrophic forgetting) Problems: in supervised learning: MNIST permutations, each task is transformed by a fixed permutation of the MNIST pixels; MNIST rotations, each task contains digits rotated by a fixed angle between 0 and 180 degrees; Omniglot, each task is one of 50 alphabets with overall 1623 characters in reinforcement learning: Catcher, a board moved left/right to catch a more and more rapidly falling object; Flappy Bird must fly between ever tightening pipes Compared against: online, same network trained straightforwardly one example at a time on the incoming non-stationary training data by simply applying SGD independent, one model per task with size of network reduced proportionally to keep total number of parameters fixed task input, trained as in online with a dedicated input layer per task EWC, Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017), ~online regularized to avoid catastrophic forgetting GEM: Gradient Episodic Memory (GEM) (Lopez-Paz &amp; Ranzato, 2017) uses episodic storage to modify gradients of latest example to not interfere with past ones; stored examples are not used in ongoing training Findings: MER seems to do learn and retain the most over all tasks, faster, and with less memory my reservations: mnist again? omniglot is not usually studied with any of the algorithms compared against: in Lake (2015) they achieve &lt;5% error rate, still &lt;15% in a stripped down version of their model and 2 out of 3 of their baselines how much slower will the training be with single example batches and two meta-learning updates?" />
<link rel="canonical" href="http://localhost:4000/2019/08/28/learing-to-learn-without-foregetting-summary.html" />
<meta property="og:url" content="http://localhost:4000/2019/08/28/learing-to-learn-without-foregetting-summary.html" />
<meta property="og:site_name" content="Lara Thompson" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2019-08-28T00:00:00-07:00" />
<script type="application/ld+json">
{"headline":"Learning to learn without forgetting: a summary","dateModified":"2019-08-28T00:00:00-07:00","url":"http://localhost:4000/2019/08/28/learing-to-learn-without-foregetting-summary.html","datePublished":"2019-08-28T00:00:00-07:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2019/08/28/learing-to-learn-without-foregetting-summary.html"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"http://localhost:4000/assets/images/y_horizon.jpg"},"name":"Lara Thompson"},"author":{"@type":"Person","name":"Lara Thompson"},"description":"Matthew Riemer et. al. Learning to Learn Without Forgetting by Maximizing Transfer and Minimizing Interference. ICLR, 2019. Recall catastrophic forgetting, a neural network sequentially trained on multiple tasks forgets earlier tasks with each new task, apparently not a problem in Bayesian networks Why? overwriting weights with updates, … How to avoid? limit weight sharing, balance network stability vs plasticity (&quot;recall of old tasks&quot; versus &quot;rapid learning of new ones&quot;), … The loss function: $$\\sum_{i, j} L(x_i , y_i ) + L(x_j , y_j ) − \\alpha {\\partial L(x_i , y_i ) \\over \\partial \\theta} \\cdot {\\partial L(x_j , y_j ) \\over \\partial \\theta}$$ The regularizing term is a measure of transfer or interference between updates. The gradient wrt to learning parameters guides the backprop update to those parameters: alignment of gradients means the updates agree and will guide learning for both examples; anti-alignment means updates cancel and neither example will learn; any intervening overlap is deemed transfer (interference) for positive (negative) values. Maximizing weight sharing maximizes transfer; minimizing weight sharing minimizes the change for interference. Work leading up to this paper, both offline algorithms over dataset D: MAML -&gt; FOMAML (Finn &amp; Levine, 2017) Reptile (Nichol &amp; Schulman, 2018) Contributions: new algorithm MER, meta experience replay, an online algorithm (algorithms 1 with variants 6 &amp; 7): added an inner loop within Reptile batches for an inner meta-learning update keeps a memory/reservoir of examples M to approximate the full dataset D with new examples added probabilistically to replace old ones (see algorithm 3 in the paper) prioritizes learning of the current examples, esp. because it may not be saved First, the reptile algorithm: for each epoch of training, \\(t\\), record the current params, \\(\\theta^A_0 = \\theta_{t-1}\\) and sample \\(s\\) batches of size \\(k\\) perform a normal epoch of training over the \\(s\\) batches with learning rate \\(\\alpha\\) toward final params \\(\\theta^A_s\\) update the network weights for this epoch only a fraction of the learned param changes: $$\\theta_t = \\theta^A_0 + \\gamma (\\theta^A_s - \\theta^A_0)$$ this meta-learning update enacts the effective loss $$2\\sum_{i=1}^s L(B_i) - \\sum_{j=1}^{i-1} {\\partial L(B_i) \\over \\partial \\theta} \\cdot {\\partial L(B_j) \\over \\partial \\theta}$$ MER adds a second meta-learning update within each of the \\(s\\) batches, now sampled from reservoir M, each of which will have the current example in it; finally, the reservoir is updated (maybe) for each epoch of training, \\(t\\), record the current params, \\(\\theta^A_0 = \\theta_{t-1}\\) and sample \\(s\\) batches of size \\(k\\), include example \\(x_t, y_t\\) in each for each batch \\(i\\), record the current params, \\(\\theta^A_{i, 0} = \\theta^A_{i-1} \\) for each example \\(j\\) in the batch, perform a backprop update with learning rate \\(\\alpha\\) to params \\(\\theta^A_{i, j}\\) after the entire batch has been singly learned, meta-learn the parameter update $$\\theta^A_i = \\theta^A_{i, 0} + \\beta (\\theta^A_{i, k} - \\theta^A_{i, 0})$$ the effective loss is $$2\\sum_{i=1}^s \\sum_{j=1}^k L(x_{ij}, y_{ij}) - \\sum_{q=1}^{i-1}\\sum_{r=1}^{j-1} {\\partial L(x_{ij}, y_{ij}) \\over \\partial \\theta} \\cdot {\\partial L(x_{qr}, y_{qr}) \\over \\partial \\theta}$$ note that they update the batch examples singly to maximize the regularizing effect algorithms 6 &amp; 7 are alternate ways of prioritizing the current example Evaluation metrics: learning accuracy (LA): average accuracy for each task immediately after it has been learned retained accuracy (RA): final retained accuracy across all tasks learned sequentially backward transfer and interference (BTI): the average change in accuracy from when a task is learned to the end of training (positive good; large and negative is catastrophic forgetting) Problems: in supervised learning: MNIST permutations, each task is transformed by a fixed permutation of the MNIST pixels; MNIST rotations, each task contains digits rotated by a fixed angle between 0 and 180 degrees; Omniglot, each task is one of 50 alphabets with overall 1623 characters in reinforcement learning: Catcher, a board moved left/right to catch a more and more rapidly falling object; Flappy Bird must fly between ever tightening pipes Compared against: online, same network trained straightforwardly one example at a time on the incoming non-stationary training data by simply applying SGD independent, one model per task with size of network reduced proportionally to keep total number of parameters fixed task input, trained as in online with a dedicated input layer per task EWC, Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017), ~online regularized to avoid catastrophic forgetting GEM: Gradient Episodic Memory (GEM) (Lopez-Paz &amp; Ranzato, 2017) uses episodic storage to modify gradients of latest example to not interfere with past ones; stored examples are not used in ongoing training Findings: MER seems to do learn and retain the most over all tasks, faster, and with less memory my reservations: mnist again? omniglot is not usually studied with any of the algorithms compared against: in Lake (2015) they achieve &lt;5% error rate, still &lt;15% in a stripped down version of their model and 2 out of 3 of their baselines how much slower will the training be with single example batches and two meta-learning updates?","@type":"BlogPosting","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=bf831e0c8f866286f030b87c8aaccbc526ced4ec">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
  </head>
  <body>
      
    <div class="wrapper">
      <header>
        
        <div class="menu">
            <a href="/index.html">home</a> |
            <a href="/posts.html">blog</a> | 
            <a href="/resources.html">resources | 
            <a href="/about">about</a>
        </div>
    <hr width=100%>
        <!--  
        <h2><a href="http://localhost:4000/">Lara Thompson</a></h2>
        <p>A blurb</p>

        -->
      </header>
      <section>

      <small>28 August 2019</small>
<h2>Learning to learn without forgetting: a summary</h2>

<p class="view">by Lara Thompson</p>

<p>Matthew Riemer et. al. <a href="https://arxiv.org/abs/1810.11910">Learning to Learn Without Forgetting by Maximizing Transfer and Minimizing Interference.</a> <em>ICLR</em>, 2019.</p>
<ul>
<li>Recall <a href="https://en.wikipedia.org/wiki/Catastrophic_interference">catastrophic forgetting</a>, a neural network sequentially trained on multiple tasks forgets earlier tasks with each new task, apparently not a problem in Bayesian networks</li>
<li>Why? overwriting weights with updates, …</li>
<li>How to avoid? limit weight sharing, balance network stability vs plasticity ("recall of old tasks" versus "rapid learning of new ones"), …</li>
<li>The loss function: $$\sum_{i, j} L(x_i , y_i ) + L(x_j , y_j ) − \alpha {\partial L(x_i , y_i ) \over \partial \theta} \cdot {\partial L(x_j , y_j ) \over \partial \theta}$$</li>
<li>The regularizing term is a measure of <em>transfer</em> or <em>interference</em> between updates. The gradient wrt to learning parameters guides the backprop update to those parameters: alignment of gradients means the updates agree and will guide learning for both examples; anti-alignment means updates cancel and neither example will learn; any intervening overlap is deemed transfer (interference) for positive (negative) values.</li>
<li>Maximizing weight sharing maximizes transfer; minimizing weight sharing minimizes the change for interference.</li>
<li>Work leading up to this paper, both offline algorithms over dataset D:
<ul>
<li>MAML -&gt; FOMAML (Finn &amp; Levine, 2017)</li>
<li>Reptile (Nichol &amp; Schulman, 2018)</li>
</ul>
</li>
<li>Contributions: new algorithm MER, meta experience replay, an online algorithm (algorithms 1 with variants 6 &amp; 7):
<ul>
<li>added an inner loop within Reptile batches for an inner meta-learning update</li>
<li>keeps a memory/reservoir of examples M to approximate the full dataset D with new examples added probabilistically to replace old ones (see algorithm 3 in the paper)</li>
<li>prioritizes learning of the current examples, esp. because it may not be saved</li>
</ul>
</li>
<li>First, the reptile algorithm:
<ul>
<li>for each epoch of training, \(t\), record the current params, \(\theta^A_0 = \theta_{t-1}\) and sample \(s\) batches of size \(k\)</li>
<li>perform a normal epoch of training over the \(s\) batches with learning rate \(\alpha\) toward final params \(\theta^A_s\)</li>
<li>update the network weights for this epoch only a fraction of the learned param changes: 
    $$\theta_t = \theta^A_0 + \gamma (\theta^A_s - \theta^A_0)$$</li>
<li>this meta-learning update enacts the effective loss 
    $$2\sum_{i=1}^s L(B_i) - \sum_{j=1}^{i-1} {\partial L(B_i) \over \partial \theta} \cdot {\partial L(B_j) \over \partial \theta}$$
</li>
</ul>
</li>
<li>MER adds a second meta-learning update within each of the \(s\) batches, now sampled from reservoir M, each of which will have the current example in it; finally, the reservoir is updated (maybe)
<ul>
<li>for each epoch of training, \(t\), record the current params, \(\theta^A_0 = \theta_{t-1}\) and sample \(s\) batches of size \(k\), include example \(x_t, y_t\) in each</li>
<li>for each batch \(i\), record the current params, \(\theta^A_{i, 0} = \theta^A_{i-1} \)</li>
<li>for each example \(j\) in the batch, perform a backprop update with learning rate 
    \(\alpha\) to params \(\theta^A_{i, j}\)</li>
<li>after the entire batch has been singly learned, meta-learn the parameter update 
    $$\theta^A_i = \theta^A_{i, 0} + \beta (\theta^A_{i, k} - \theta^A_{i, 0})$$</li>
<li>the effective loss is 
    $$2\sum_{i=1}^s \sum_{j=1}^k L(x_{ij}, y_{ij}) - \sum_{q=1}^{i-1}\sum_{r=1}^{j-1} {\partial L(x_{ij}, y_{ij}) \over \partial \theta} \cdot {\partial L(x_{qr}, y_{qr}) \over \partial \theta}$$</li>
<li>note that they update the batch examples singly to maximize the regularizing effect</li>
<li>algorithms 6 &amp; 7 are alternate ways of prioritizing the current example</li>
</ul>
</li>
<li>Evaluation metrics:
<ul>
<li>learning accuracy (LA): average accuracy for each task immediately after it has been learned</li>
<li>retained accuracy (RA): final retained accuracy across all tasks learned sequentially</li>
<li>backward transfer and interference (BTI): the average change in accuracy from when a task is learned to the end of training (positive good; large and negative is catastrophic forgetting)</li>
</ul>
</li>
<li>Problems:
<ul>
<li>in supervised learning: MNIST permutations, each task is transformed by a fixed permutation of the MNIST pixels; MNIST rotations, each task contains digits rotated by a fixed angle between 0 and 180 degrees; Omniglot, each task is one of 50 alphabets with overall 1623 characters</li>
<li>in reinforcement learning: Catcher, a board moved left/right to catch a more and more rapidly falling object; Flappy Bird must fly between ever tightening pipes</li>
</ul>
</li>
<li>Compared against:
<ul>
<li>online, same network trained straightforwardly one example at a time on the incoming non-stationary training data by simply applying SGD</li>
<li>independent, one model per task with size of network reduced proportionally to keep total number of parameters fixed</li>
<li>task input, trained as in online with a dedicated input layer per task</li>
<li>EWC, Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017), ~online regularized to avoid catastrophic forgetting</li>
<li>GEM: Gradient Episodic Memory (GEM) (Lopez-Paz &amp; Ranzato, 2017) uses episodic storage to modify gradients of latest example to not interfere with past ones; stored examples are not used in ongoing training
Findings:</li>
<li>MER seems to do learn and retain the most over all tasks, faster, and with less memory</li>
<li>my reservations:
<ul>
  <li>mnist again?</li>
  <li>omniglot is not usually studied with any of the algorithms compared against: in Lake (2015) they achieve &lt;5% error rate, still &lt;15% in a stripped down version of their model and 2 out of 3 of their baselines</li>
  <li>how much slower will the training be with single example batches and two meta-learning updates?</li>
</ul>
</li>
</ul>
</li>
</ul>



  <small>tags: <em>paper summary</em> - <em>data science</em></small>



      </section>
    
      <footer>
        
        <span class="credits left">Project maintained by <a href="https://github.com/lrthomps">lrthomps</a></span>
          <br>
        
      <span class="credits right">Hosted on GitHub Pages &mdash; Midnight by <a href="https://twitter.com/mattgraham">mattgraham</a></span>
      </footer>
        
    </div>
    <script src="/assets/js/scale.fix.js"></script>
    
  </body>
</html>
