---
layout: default
---


I'm a believer in <a href="https://www.goodreads.com/book/show/25744928-deep-work">Deep Work</a>, known also as deliberate
practice, made famous in Gladwell's Outliers, better explained in Ericsson's Peak: practicing what's hardest with focus for immediate feedback. 
Feedback can be the hardest part but, in machine learning, the generalization (tested fairly) doesn't lie. Next hardest, is
fairly assessing where you are weakest. Be honest. 

I realized recently that I was fairly versed in supervised and unsupervised
learning and yet the only aspect of reinforcement learning I knew was exploit vs explore. But after picking
up Sutton's book, notions in RL jumped out everywhere! (Fuzzy) A/B testing leveraging counterfactuals is akin to off-policy
learning. 

Some topics to expect here:
* continual learning vs one-shot learning
* attention: how and when it can be used, what forms it can take, the important elements of a minimal description
* explainable machine learning 
* study notes of some of the books in resources
* details on projects I'm working on

The next things on my to-study list:
* explainability of black-box models: what lies after LIME and SHAP? Causal Bayesian Networks?
* recurrent neural networks: does attention really make them obsolete? Can we get memory into feed-forward networks
akin to LSTM?
* game theory (to deepen my understanding of RL)
* cryptography, as a facet of privacy

Suggestions?


