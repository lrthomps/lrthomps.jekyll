---
layout: default
---


Studied math and physics, now a data scientist, in Vancouver, BC.

In Vancouver because I love the mountains so near, prefer my snow up there, don't mind the rain down here, and could not live in an un-walkable/un-bikeable place.

Machine learning because it felt natural after physics, closer to what's important in everyday life, exciting in how powerful it can be, challenging in how badly it can fail.

Data Science because it combines math, programming, and problem solving always in context of what is *really* important, be that the real world, a constrained infrastructure, or a fickle market.

I care deeply about people and the environment. We need products and services that help people, not those that manipulate or merely distract. Technology that means less waste and not just more stuff. 

Recommenders that connect people with what they might love but would never have found otherwise.
Translations that reach across languages. Speech to text that loves accents as much I do. Models that expose bias and can go beyond. Data that can stay private and still be used for training.

There's so much potential and so much to learn. I link to things that I find most helpful in <a href="{{ "resources.html" | relative_url }}">üçå</a>; in the <a href="{{ "posts.html" | relative_url }}">blog</a>, I'll explore topics that interest me. The posts are mostly for my own reference but feel free to write if you find something interesting or confusing. I'm best contacted as *laraph* on <a href="https://vantech.slack.com">vantech.slack.com</a> (<a href="https://vantech.herokuapp.com/">vantech.herokuapp.com</a> to sign up).

<hr>

Some topics to expect here:
* continual learning vs one-shot learning
* attention: how and when it can be used, what forms it can take, the important elements of a minimal description
* explainable machine learning 
* study notes of some of the books in resources
* details on projects I'm working on

The next things on my to-study list:
* explainability of black-box models: what lies after LIME and SHAP? Causal Bayesian Networks?
* recurrent neural networks: does attention really make them obsolete? Can we get memory into feed-forward networks
akin to LSTM?
* game theory (to deepen my understanding of RL)
* cryptography, as a facet of privacy

Suggestions?


